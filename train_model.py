import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("FCN å ±åƒ¹é æ¸¬æ¨¡å‹è¨“ç·´")
print("=" * 80)

# ============================================================================
# 1. è¼‰å…¥è³‡æ–™
# ============================================================================
print("\n" + "=" * 80)
print("1. è¼‰å…¥è³‡æ–™")
print("=" * 80)

df = pd.read_excel('FCN_features_v3_sorted.xlsx')
print(f"è³‡æ–™å½¢ç‹€: {df.shape}")

# ============================================================================
# 2. ç‰¹å¾µé¸æ“‡
# ============================================================================
print("\n" + "=" * 80)
print("2. ç‰¹å¾µé¸æ“‡")
print("=" * 80)

# å®šç¾©ç›®æ¨™è®Šæ•¸
target = 'Coupon'

# å®šç¾©ä¸æ‡‰è©²ä½œç‚ºç‰¹å¾µçš„æ¬„ä½
exclude_cols = [
    # ç›®æ¨™è®Šæ•¸
    'Coupon', 'Coupon p.a. (%)', 'Coupon_Valid',
    # æ—¥æœŸå’Œè­˜åˆ¥æ¬„ä½
    'Pricing Date',
    # åŸå§‹BBG Code (é¡åˆ¥å‹ï¼Œéœ€è¦ç‰¹æ®Šè™•ç†)
    'BBG Code 1', 'BBG Code 2', 'BBG Code 3',
    # å¸¸æ•¸æ¬„ä½ (åªæœ‰ä¸€å€‹å€¼)
    'Product', 'Currency', 'KO Type',
    # åŸå§‹æœªæ’åºçš„IVæ¬„ä½ (ä½¿ç”¨æ’åºå¾Œçš„ç‰ˆæœ¬)
    'PUT_IMP_VOL_3M', 'PUT_IMP_VOL_3M_2', 'PUT_IMP_VOL_3M_3',
    'VOLATILITY_90D', 'VOLATILITY_90D_2', 'VOLATILITY_90D_3',
    'CALL_IMP_VOL_2M_25D', 'CALL_IMP_VOL_2M_25D_2', 'CALL_IMP_VOL_2M_25D_3',
    'PUT_IMP_VOL_2M_25D', 'PUT_IMP_VOL_2M_25D_2', 'PUT_IMP_VOL_2M_25D_3',
    'HIST_PUT_IMP_VOL', 'HIST_PUT_IMP_VOL_2', 'HIST_PUT_IMP_VOL_3',
    'VOL_STDDEV', 'VOL_STDDEV_2', 'VOL_STDDEV_3',
    'VOL_PERCENTILE', 'VOL_PERCENTILE_2', 'VOL_PERCENTILE_3',
    'CHG_PCT_1YR', 'CHG_PCT_1YR_2', 'CHG_PCT_1YR_3',
    'CORR_COEF', 'CORR_COEF_2', 'CORR_COEF_3',
    'DIVIDEND_YIELD', 'DIVIDEND_YIELD_2', 'DIVIDEND_YIELD_3',
    'PX_LAST', 'PX_LAST_2', 'PX_LAST_3',
    'IV_Skew_1', 'IV_Skew_2', 'IV_Skew_3',
    'IV_Premium_1', 'IV_Premium_2', 'IV_Premium_3',
    # èˆŠçš„èšåˆç‰¹å¾µ (ä½¿ç”¨æ’åºå¾Œçš„æ›´ç²¾ç¢º)
    'Avg_IV_3M', 'Max_IV_3M', 'Min_IV_3M', 'Avg_Historical_Vol_90D',
    'Basket_Worst_IV', 'Basket_Best_IV', 'Basket_Avg_IV',
    'Basket_Worst_HV', 'Basket_Best_HV', 'Basket_Avg_HV',
]

# ç²å–æ‰€æœ‰æ•¸å€¼å‹ç‰¹å¾µ
all_numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# éæ¿¾å‡ºè¦ä½¿ç”¨çš„ç‰¹å¾µ
feature_cols = [col for col in all_numeric_cols if col not in exclude_cols]

print(f"ç¸½æ¬„ä½æ•¸: {len(df.columns)}")
print(f"æ’é™¤æ¬„ä½æ•¸: {len(exclude_cols)}")
print(f"ä½¿ç”¨ç‰¹å¾µæ•¸: {len(feature_cols)}")

# è™•ç†é¡åˆ¥è®Šæ•¸
print("\nã€é¡åˆ¥è®Šæ•¸è™•ç†ã€‘")
# Barrier Type å·²ç¶“æœ‰ Barrier_Type_AKI çš„ç·¨ç¢¼

# é¡¯ç¤ºæœ€çµ‚ä½¿ç”¨çš„ç‰¹å¾µ
print("\nã€ä½¿ç”¨çš„ç‰¹å¾µåˆ—è¡¨ã€‘")
for i, col in enumerate(sorted(feature_cols), 1):
    print(f"  {i:3d}. {col}")

# ============================================================================
# 3. æº–å‚™è¨“ç·´è³‡æ–™
# ============================================================================
print("\n" + "=" * 80)
print("3. æº–å‚™è¨“ç·´è³‡æ–™")
print("=" * 80)

X = df[feature_cols].copy()
y = df[target].copy()

print(f"X shape: {X.shape}")
print(f"y shape: {y.shape}")

# æª¢æŸ¥ç¼ºå¤±å€¼
print(f"\nã€ç‰¹å¾µç¼ºå¤±å€¼çµ±è¨ˆã€‘")
missing_stats = X.isnull().sum()
missing_features = missing_stats[missing_stats > 0].sort_values(ascending=False)
print(f"æœ‰ç¼ºå¤±å€¼çš„ç‰¹å¾µæ•¸: {len(missing_features)}")
if len(missing_features) > 0:
    print("\nTop 10 ç¼ºå¤±ç‰¹å¾µ:")
    print(missing_features.head(10))

# åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"\nè¨“ç·´é›†: {X_train.shape}")
print(f"æ¸¬è©¦é›†: {X_test.shape}")

# ============================================================================
# 4. è¨“ç·´æ¨¡å‹
# ============================================================================
print("\n" + "=" * 80)
print("4. è¨“ç·´æ¨¡å‹")
print("=" * 80)

# å˜—è©¦å°å…¥XGBoost
try:
    import xgboost as xgb
    HAS_XGB = True
    print("âœ… XGBoost å¯ç”¨")
except ImportError:
    HAS_XGB = False
    print("âŒ XGBoost ä¸å¯ç”¨")

# å˜—è©¦å°å…¥LightGBM
try:
    import lightgbm as lgb
    HAS_LGB = True
    print("âœ… LightGBM å¯ç”¨")
except ImportError:
    HAS_LGB = False
    print("âŒ LightGBM ä¸å¯ç”¨")

# å°å…¥åŸºæœ¬æ¨¡å‹
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

results = {}

# 4.1 HistGradient Boosting (æ”¯æ´NaNï¼Œä½œç‚ºåŸºæº–)
print("\nã€4.1 HistGradient Boosting (sklearn, æ”¯æ´NaN)ã€‘")
from sklearn.ensemble import HistGradientBoostingRegressor

hgb_model = HistGradientBoostingRegressor(
    max_iter=300,
    max_depth=10,
    learning_rate=0.1,
    min_samples_leaf=5,
    random_state=42
)
hgb_model.fit(X_train, y_train)
hgb_pred = hgb_model.predict(X_test)

hgb_rmse = np.sqrt(mean_squared_error(y_test, hgb_pred))
hgb_mae = mean_absolute_error(y_test, hgb_pred)
hgb_r2 = r2_score(y_test, hgb_pred)

print(f"  RMSE: {hgb_rmse:.4f}")
print(f"  MAE:  {hgb_mae:.4f}")
print(f"  RÂ²:   {hgb_r2:.4f}")

results['HistGradient Boosting'] = {'RMSE': hgb_rmse, 'MAE': hgb_mae, 'R2': hgb_r2, 'model': hgb_model}

# 4.2 HistGradient Boosting (æ›´æ·±çš„æ¨¹)
print("\nã€4.2 HistGradient Boosting (æ·±å±¤)ã€‘")

hgb_deep_model = HistGradientBoostingRegressor(
    max_iter=500,
    max_depth=15,
    learning_rate=0.05,
    min_samples_leaf=3,
    random_state=42
)
hgb_deep_model.fit(X_train, y_train)
hgb_deep_pred = hgb_deep_model.predict(X_test)

hgb_deep_rmse = np.sqrt(mean_squared_error(y_test, hgb_deep_pred))
hgb_deep_mae = mean_absolute_error(y_test, hgb_deep_pred)
hgb_deep_r2 = r2_score(y_test, hgb_deep_pred)

print(f"  RMSE: {hgb_deep_rmse:.4f}")
print(f"  MAE:  {hgb_deep_mae:.4f}")
print(f"  RÂ²:   {hgb_deep_r2:.4f}")

results['HistGradient Boosting Deep'] = {'RMSE': hgb_deep_rmse, 'MAE': hgb_deep_mae, 'R2': hgb_deep_r2, 'model': hgb_deep_model}

# 4.3 XGBoost
if HAS_XGB:
    print("\nã€4.3 XGBoostã€‘")
    xgb_model = xgb.XGBRegressor(
        n_estimators=300,
        max_depth=8,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=-1,
        verbosity=0
    )
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_test)

    xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
    xgb_mae = mean_absolute_error(y_test, xgb_pred)
    xgb_r2 = r2_score(y_test, xgb_pred)

    print(f"  RMSE: {xgb_rmse:.4f}")
    print(f"  MAE:  {xgb_mae:.4f}")
    print(f"  RÂ²:   {xgb_r2:.4f}")

    results['XGBoost'] = {'RMSE': xgb_rmse, 'MAE': xgb_mae, 'R2': xgb_r2, 'model': xgb_model}

# 4.4 LightGBM
if HAS_LGB:
    print("\nã€4.4 LightGBMã€‘")
    lgb_model = lgb.LGBMRegressor(
        n_estimators=300,
        max_depth=8,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        n_jobs=-1,
        verbosity=-1
    )
    lgb_model.fit(X_train, y_train)
    lgb_pred = lgb_model.predict(X_test)

    lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred))
    lgb_mae = mean_absolute_error(y_test, lgb_pred)
    lgb_r2 = r2_score(y_test, lgb_pred)

    print(f"  RMSE: {lgb_rmse:.4f}")
    print(f"  MAE:  {lgb_mae:.4f}")
    print(f"  RÂ²:   {lgb_r2:.4f}")

    results['LightGBM'] = {'RMSE': lgb_rmse, 'MAE': lgb_mae, 'R2': lgb_r2, 'model': lgb_model}

# ============================================================================
# 5. æ¨¡å‹æ¯”è¼ƒ
# ============================================================================
print("\n" + "=" * 80)
print("5. æ¨¡å‹æ¯”è¼ƒ")
print("=" * 80)

print("\nã€å„æ¨¡å‹è¡¨ç¾ã€‘")
print(f"{'æ¨¡å‹':<20} {'RMSE':>10} {'MAE':>10} {'RÂ²':>10}")
print("-" * 52)
for name, metrics in sorted(results.items(), key=lambda x: x[1]['R2'], reverse=True):
    print(f"{name:<20} {metrics['RMSE']:>10.4f} {metrics['MAE']:>10.4f} {metrics['R2']:>10.4f}")

# é¸æ“‡æœ€ä½³æ¨¡å‹
best_model_name = max(results.keys(), key=lambda x: results[x]['R2'])
best_model = results[best_model_name]['model']
best_r2 = results[best_model_name]['R2']

print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (RÂ² = {best_r2:.4f})")

# ============================================================================
# 6. äº¤å‰é©—è­‰
# ============================================================================
print("\n" + "=" * 80)
print("6. äº¤å‰é©—è­‰ (5-Fold)")
print("=" * 80)

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

for name, metrics in results.items():
    model = metrics['model']
    cv_scores = cross_val_score(model, X, y, cv=kfold, scoring='r2', n_jobs=-1)
    print(f"{name:<20} CV RÂ² = {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")

# ============================================================================
# 7. ç‰¹å¾µé‡è¦æ€§åˆ†æ
# ============================================================================
print("\n" + "=" * 80)
print("7. ç‰¹å¾µé‡è¦æ€§åˆ†æ")
print("=" * 80)

# ä½¿ç”¨æœ€ä½³æ¨¡å‹çš„ç‰¹å¾µé‡è¦æ€§
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print(f"\nã€{best_model_name} ç‰¹å¾µé‡è¦æ€§ Top 25ã€‘")
    print(f"{'æ’å':<5} {'ç‰¹å¾µåç¨±':<40} {'é‡è¦æ€§':>10} {'ä½”æ¯”':>10}")
    print("-" * 70)

    total_importance = feature_importance['importance'].sum()
    cumulative = 0
    for i, row in feature_importance.head(25).iterrows():
        rank = feature_importance.index.get_loc(i) + 1
        pct = row['importance'] / total_importance * 100
        cumulative += pct
        print(f"{rank:<5} {row['feature']:<40} {row['importance']:>10.4f} {pct:>9.2f}%")

    print(f"\nTop 25 ç‰¹å¾µç´¯ç©é‡è¦æ€§: {cumulative:.2f}%")

    # å„²å­˜ç‰¹å¾µé‡è¦æ€§
    feature_importance.to_excel('feature_importance.xlsx', index=False)
    print("\nç‰¹å¾µé‡è¦æ€§å·²å„²å­˜è‡³: feature_importance.xlsx")

# ============================================================================
# 8. é æ¸¬èª¤å·®åˆ†æ
# ============================================================================
print("\n" + "=" * 80)
print("8. é æ¸¬èª¤å·®åˆ†æ")
print("=" * 80)

best_pred = best_model.predict(X_test)
errors = y_test - best_pred

print(f"\nã€èª¤å·®åˆ†ä½ˆã€‘")
print(f"å¹³å‡èª¤å·® (Mean Error): {errors.mean():.4f}")
print(f"èª¤å·®æ¨™æº–å·® (Std):      {errors.std():.4f}")
print(f"æœ€å¤§é«˜ä¼°:              {errors.min():.4f}")
print(f"æœ€å¤§ä½ä¼°:              {errors.max():.4f}")

# èª¤å·®ç™¾åˆ†ä½æ•¸
print(f"\nã€èª¤å·®ç™¾åˆ†ä½æ•¸ã€‘")
for pct in [5, 25, 50, 75, 95]:
    print(f"  {pct}%: {np.percentile(errors, pct):.4f}")

# çµ•å°èª¤å·®åˆ†ä½ˆ
abs_errors = np.abs(errors)
print(f"\nã€çµ•å°èª¤å·®åˆ†ä½ˆã€‘")
print(f"å¹³å‡çµ•å°èª¤å·® (MAE): {abs_errors.mean():.4f}")
print(f"  < 0.5%:  {(abs_errors < 0.5).sum() / len(abs_errors) * 100:.1f}%")
print(f"  < 1.0%:  {(abs_errors < 1.0).sum() / len(abs_errors) * 100:.1f}%")
print(f"  < 2.0%:  {(abs_errors < 2.0).sum() / len(abs_errors) * 100:.1f}%")
print(f"  < 3.0%:  {(abs_errors < 3.0).sum() / len(abs_errors) * 100:.1f}%")

# ============================================================================
# 9. å„²å­˜æ¨¡å‹
# ============================================================================
print("\n" + "=" * 80)
print("9. å„²å­˜æ¨¡å‹")
print("=" * 80)

import joblib

# å„²å­˜æœ€ä½³æ¨¡å‹
model_filename = f'fcn_model_{best_model_name.lower().replace(" ", "_")}.pkl'
joblib.dump(best_model, model_filename)
print(f"æ¨¡å‹å·²å„²å­˜è‡³: {model_filename}")

# å„²å­˜ç‰¹å¾µåˆ—è¡¨
feature_list_filename = 'model_features.txt'
with open(feature_list_filename, 'w') as f:
    for feat in feature_cols:
        f.write(f"{feat}\n")
print(f"ç‰¹å¾µåˆ—è¡¨å·²å„²å­˜è‡³: {feature_list_filename}")

# å„²å­˜é æ¸¬çµæœæ¯”è¼ƒ
prediction_df = pd.DataFrame({
    'Actual': y_test.values,
    'Predicted': best_pred,
    'Error': errors.values,
    'Abs_Error': abs_errors.values
})
prediction_df.to_excel('prediction_results.xlsx', index=False)
print(f"é æ¸¬çµæœå·²å„²å­˜è‡³: prediction_results.xlsx")

# ============================================================================
# 10. ç¸½çµ
# ============================================================================
print("\n" + "=" * 80)
print("10. æ¨¡å‹è¨“ç·´ç¸½çµ")
print("=" * 80)

print(f"""
ğŸ“Š è³‡æ–™è¦æ¨¡:
   - è¨“ç·´æ¨£æœ¬: {len(X_train)}
   - æ¸¬è©¦æ¨£æœ¬: {len(X_test)}
   - ç‰¹å¾µæ•¸é‡: {len(feature_cols)}

ğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}
   - RÂ²:   {results[best_model_name]['R2']:.4f}
   - RMSE: {results[best_model_name]['RMSE']:.4f}
   - MAE:  {results[best_model_name]['MAE']:.4f}

ğŸ“ˆ é æ¸¬æº–ç¢ºåº¦:
   - {(abs_errors < 1.0).sum() / len(abs_errors) * 100:.1f}% çš„é æ¸¬èª¤å·® < 1%
   - {(abs_errors < 2.0).sum() / len(abs_errors) * 100:.1f}% çš„é æ¸¬èª¤å·® < 2%

ğŸ’¾ è¼¸å‡ºæª”æ¡ˆ:
   - {model_filename}: è¨“ç·´å¥½çš„æ¨¡å‹
   - model_features.txt: ç‰¹å¾µåˆ—è¡¨
   - feature_importance.xlsx: ç‰¹å¾µé‡è¦æ€§
   - prediction_results.xlsx: é æ¸¬çµæœ
""")

print("=" * 80)
print("æ¨¡å‹è¨“ç·´å®Œæˆï¼")
print("=" * 80)
